{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a29ecdf7",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<tokenize>, line 191)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<tokenize>\"\u001b[1;36m, line \u001b[1;32m191\u001b[0m\n\u001b[1;33m    epochs=2,\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import random\n",
    "import shutil\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from shutil import copyfile\n",
    "from os import getcwd\n",
    "from os import listdir\n",
    "import cv2\n",
    "from tensorflow.keras.layers import Conv2D, Input, ZeroPadding2D, BatchNormalization, Activation, MaxPooling2D, Flatten, Dense\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.utils import shuffle\n",
    "import imutils\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image  as mpimg\n",
    "\n",
    "print(len(os.listdir('yes')))\n",
    "print(len(os.listdir('no')))\n",
    "try:\n",
    "    os.mkdir('trial1')\n",
    "    os.mkdir('trial1/augmented data1/')\n",
    "    os.mkdir('trial1/augmented data1/training')\n",
    "    os.mkdir('trial1/augmented data1/training/yes1')\n",
    "    os.mkdir('trial1/augmented data1/training/no1')\n",
    "    os.mkdir('trial1/augmented data1/testing')\n",
    "    os.mkdir('trial1/augmented data1/testing/yes1')\n",
    "    os.mkdir('trial1/augmented data1/testing/no1')\n",
    "    os.mkdir('trial1/augmented data1/yesreal')\n",
    "    os.mkdir('trial1/augmented data1/noreal')\n",
    "except OSError:\n",
    "    pass\n",
    "\n",
    "\n",
    "\n",
    "def augment_data(file_dir, n_generated_samples, save_to_dir):\n",
    "    #from keras.preprocessing.image import ImageDataGenerator\n",
    "    #from os import listdir\n",
    "    \n",
    "    data_gen = ImageDataGenerator(rotation_range=10, \n",
    "                                  width_shift_range=0.1, \n",
    "                                  height_shift_range=0.1, \n",
    "                                  shear_range=0.1, \n",
    "                                  brightness_range=(0.3, 1.0),\n",
    "                                  horizontal_flip=True, \n",
    "                                  vertical_flip=True, \n",
    "                                  fill_mode='nearest'\n",
    "                                 )\n",
    "\n",
    "    \n",
    "    for filename in listdir(file_dir):\n",
    "        # load the image\n",
    "        image = cv2.imread(file_dir + '\\\\' + filename)\n",
    "        # reshape the image\n",
    "        image = image.reshape((1,)+image.shape)\n",
    "        # prefix of the names for the generated sampels.\n",
    "        save_prefix = 'aug_' + filename[:-4]\n",
    "        # generate 'n_generated_samples' sample images\n",
    "        i=0\n",
    "        for batch in data_gen.flow(x=image, batch_size=1, save_to_dir=save_to_dir, \n",
    "                                           save_prefix=save_prefix, save_format='jpg'):\n",
    "            i += 1\n",
    "            if i > n_generated_samples:\n",
    "                break\n",
    "\n",
    "\n",
    "\n",
    "augmented_data_path = 'trial1/augmented data1/'\n",
    "\n",
    "# augment data for the examples with label equal to 'yes' representing tumurous examples\n",
    "augment_data(file_dir='yes', n_generated_samples=6, save_to_dir=augmented_data_path+'yesreal')\n",
    "# augment data for the examples with label equal to 'no' representing non-tumurous examples\n",
    "augment_data(file_dir='no', n_generated_samples=9, save_to_dir=augmented_data_path+'noreal')\n",
    "\n",
    "\n",
    "def data_summary(main_path):\n",
    "    \n",
    "    yes_path = main_path+'yesreal'\n",
    "    no_path = main_path+'noreal'\n",
    "        \n",
    "    # number of files (images) that are in the the folder named 'yes' that represent tumorous (positive) examples\n",
    "    m_pos = len(listdir(yes_path))\n",
    "    # number of files (images) that are in the the folder named 'no' that represent non-tumorous (negative) examples\n",
    "    m_neg = len(listdir(no_path))\n",
    "    # number of all examples\n",
    "    m = (m_pos+m_neg)\n",
    "    \n",
    "    pos_prec = (m_pos* 100.0)/ m\n",
    "    neg_prec = (m_neg* 100.0)/ m\n",
    "    \n",
    "    print(f\"Number of examples: {m}\")\n",
    "    print(f\"Percentage of positive examples: {pos_prec}%, number of pos examples: {m_pos}\") \n",
    "    print(f\"Percentage of negative examples: {neg_prec}%, number of neg examples: {m_neg}\") \n",
    "    \n",
    "    \n",
    "data_summary(augmented_data_path)\n",
    "\n",
    "\n",
    "\n",
    "def split_data(SOURCE, TRAINING, TESTING, SPLIT_SIZE):\n",
    "    dataset = []\n",
    "    \n",
    "    for unitData in os.listdir(SOURCE):\n",
    "        data = SOURCE + unitData\n",
    "        if(os.path.getsize(data) > 0):\n",
    "            dataset.append(unitData)\n",
    "        else:\n",
    "            print('Skipped ' + unitData)\n",
    "            print('Invalid file i.e zero size')\n",
    "    \n",
    "    train_set_length = int(len(dataset) * SPLIT_SIZE)\n",
    "    test_set_length = int(len(dataset) - train_set_length)\n",
    "    shuffled_set = random.sample(dataset, len(dataset))\n",
    "    train_set = dataset[0:train_set_length]\n",
    "    test_set = dataset[-test_set_length:]\n",
    "       \n",
    "    for unitData in train_set:\n",
    "        temp_train_set = SOURCE + unitData\n",
    "        final_train_set = TRAINING + unitData\n",
    "        copyfile(temp_train_set, final_train_set)\n",
    "    \n",
    "    for unitData in test_set:\n",
    "        temp_test_set = SOURCE + unitData\n",
    "        final_test_set = TESTING + unitData\n",
    "        copyfile(temp_test_set, final_test_set)\n",
    "        \n",
    "        \n",
    "YES_SOURCE_DIR = \"trial1/augmented data1/yesreal/\"\n",
    "TRAINING_YES_DIR = \"trial1/augmented data1/training/yes1/\"\n",
    "TESTING_YES_DIR = \"trial1/augmented data1/testing/yes1/\"\n",
    "NO_SOURCE_DIR = \"trial1/augmented data1/noreal/\"\n",
    "TRAINING_NO_DIR = \"trial1/augmented data1/training/no1/\"\n",
    "TESTING_NO_DIR = \"trial1/augmented data1/testing/no1/\"\n",
    "split_size = .8\n",
    "split_data(YES_SOURCE_DIR, TRAINING_YES_DIR, TESTING_YES_DIR, split_size)\n",
    "split_data(NO_SOURCE_DIR, TRAINING_NO_DIR, TESTING_NO_DIR, split_size)\n",
    "\n",
    "\n",
    "print(len(os.listdir('trial1/augmented data1/training/yes1')))\n",
    "print(len(os.listdir('trial1/augmented data1/testing/yes1')))\n",
    "print(len(os.listdir('trial1/augmented data1/training/no1')))\n",
    "print(len(os.listdir('trial1/augmented data1/testing/no1')))\n",
    "\n",
    "\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(16, (3,3), activation='relu', input_shape=(150, 150, 3)),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    \n",
    "    tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    \n",
    "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    \n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(512, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "model.compile(optimizer='adam'), loss='binary_crossentropy', metrics=['acc'])\n",
    "\n",
    "\n",
    "TRAINING_DIR = \"trial1/augmented data1/training\"\n",
    "train_datagen = ImageDataGenerator(rescale=1.0/255,\n",
    "                                   rotation_range=40,\n",
    "                                   width_shift_range=0.2,\n",
    "                                   height_shift_range=0.2,\n",
    "                                   shear_range=0.2,\n",
    "                                   zoom_range=0.2,\n",
    "                                   horizontal_flip=True,\n",
    "                                   fill_mode='nearest')\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(TRAINING_DIR, \n",
    "                                                    batch_size=10, \n",
    "                                                    class_mode='binary', \n",
    "                                                    target_size=(150, 150))\n",
    "VALIDATION_DIR = \"trial1/augmented data1/testing\"\n",
    "validation_datagen = ImageDataGenerator(rescale=1.0/255)\n",
    "\n",
    "validation_generator = validation_datagen.flow_from_directory(VALIDATION_DIR, \n",
    "                                                         batch_size=10, \n",
    "                                                         class_mode='binary', \n",
    "                                                         target_size=(150, 150))\n",
    "\n",
    "history = model.fit_generator(train_generator,\n",
    "                              epochs=2,\n",
    "                              verbose=1,\n",
    "                              validation_data=validation_generator)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "acc=history.history['acc']\n",
    "val_acc=history.history['val_acc']\n",
    "loss=history.history['loss']\n",
    "val_loss=history.history['val_loss']\n",
    "\n",
    "epochs=range(len(acc)) # Get number of epochs\n",
    "\n",
    "\n",
    "plt.plot(epochs, acc, 'r', \"Training Accuracy\")\n",
    "plt.plot(epochs, val_acc, 'b', \"Validation Accuracy\")\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.figure()\n",
    "\n",
    "\n",
    "plt.plot(epochs, loss, 'r', \"Training Loss\")\n",
    "plt.plot(epochs, val_loss, 'b', \"Validation Loss\")\n",
    "\n",
    "\n",
    "plt.title('Training and validation loss')\n",
    "\n",
    "\n",
    "from keras.preprocessing import image\n",
    "import cv2\n",
    "import os\n",
    "import glob\n",
    "img_dir = \"trial1/augmented data1/testing/no1\"\n",
    "data_path = os.path.join(img_dir,'*g')\n",
    "files = glob.glob(data_path)\n",
    "data = []\n",
    "result = []\n",
    "for f1 in files:\n",
    "    test_image = image.load_img(f1, target_size = (150, 150))\n",
    "    test_image = image.img_to_array(test_image)\n",
    "    test_image = np.expand_dims(test_image, axis = 0)\n",
    "    images = np.vstack([test_image])\n",
    "    classes = model.predict(images, batch_size=10)\n",
    "    classes = np.round(classes)\n",
    "    data.append(f1)\n",
    "    result.append(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1da8ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
